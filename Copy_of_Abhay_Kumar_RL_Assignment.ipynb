{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c68be25a",
      "metadata": {
        "id": "c68be25a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.animation as animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b274a60",
      "metadata": {
        "id": "7b274a60"
      },
      "outputs": [],
      "source": [
        "!pip install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6303f28d",
      "metadata": {
        "id": "6303f28d"
      },
      "outputs": [],
      "source": [
        "class AbstractAgent:\n",
        "    '''Abstract class for all agents. '''\n",
        "\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def step(self):\n",
        "        '''Take an action '''\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef6661f",
      "metadata": {
        "id": "6ef6661f"
      },
      "outputs": [],
      "source": [
        "class AbstractEnvironment:\n",
        "    '''Base class for all enviornments.'''\n",
        "\n",
        "    def reset(self):\n",
        "        '''Reset the state of the environment to original.\n",
        "\n",
        "        Returns:\n",
        "        The new state\n",
        "        '''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def step(self, action):\n",
        "        '''Run the enivornment one step by taking an action.\n",
        "\n",
        "        Arguments:\n",
        "        action - The action to take\n",
        "\n",
        "        Returns:\n",
        "        The amount of reward recieved\n",
        "        '''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_num_states(self):\n",
        "        '''Get the number of states in the environment'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_num_actions(self):\n",
        "        '''Get the number of actions the agent can perform'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_current_state(self):\n",
        "        '''Get the current state of the environment.'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_state_space(self):\n",
        "        '''Get the possible states of the environment'''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_action_space(self):\n",
        "        '''Get the possible actions in the environment'''\n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d18fe305",
      "metadata": {
        "id": "d18fe305"
      },
      "outputs": [],
      "source": [
        "class GridMaze(AbstractEnvironment):\n",
        "    #TODO make a 5X5 grid\n",
        "    default_maze = [\"#G.\",\n",
        "                    \"##.\",\n",
        "                    \"..A\"]\n",
        "    action_movements = {\n",
        "        0: np.array((-1, 0)),\n",
        "        #TODO map the remaining set of actions to Integer domain\n",
        "    }\n",
        "\n",
        "    def __init__(self, maze=None):\n",
        "        if maze is None:\n",
        "            maze = self.default_maze.copy()\n",
        "        self.maze = np.array(list(map(list, maze)))\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        #TODO move the agent back to its starting position\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        # if action not in self.action_movements.keys():\n",
        "        # raise ValueError(f\"Invalid action {action}. Please use N, W, S or E.\")\n",
        "\n",
        "\n",
        "        #TODO determine the new position of the agent\n",
        "        new_pos=\n",
        "\n",
        "        # TODO update new position when the agent reaches the border\n",
        "        new_pos =\n",
        "\n",
        "        # TODO update new position when the agent hits an obstucle next\n",
        "        new_pos =\n",
        "\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        #TODO what happens when the agent reaches the GOAL ?\n",
        "        #TODO update reward and grid\n",
        "        if new_pos_char == \"G\":\n",
        "            reward=\n",
        "\n",
        "        else:\n",
        "            reward=\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def get_current_state(self):\n",
        "        return tuple(self.agent_pos)\n",
        "\n",
        "    def get_state_space(self):\n",
        "        return list(np.ndindex(self.maze.shape))\n",
        "\n",
        "    def get_action_space(self):\n",
        "        return list(self.action_movements.keys())\n",
        "\n",
        "    def get_num_states(self):\n",
        "        return len(self.get_state_space())\n",
        "\n",
        "    def get_num_actions(self):\n",
        "        return len(self.get_action_space())\n",
        "\n",
        "    def render(self):\n",
        "        'plot the maze'\n",
        "        fig, ax = plt.subplots(1, 1)\n",
        "        ax.imshow(self.maze == \"#\", vmin=0, vmax=1, cmap=\"Greys\")\n",
        "        h, w = self.maze.shape\n",
        "        ax.plot([-0.5, -0.5, w - 0.5, w - 0.5, -0.5],\n",
        "                [-0.5, h - 0.5, h - 0.5, -0.5, -0.5], 'k', lw=3)\n",
        "        y, x = self.agent_pos\n",
        "        self.agent_dot, = plt.plot((x,), (y,), 'ro', ms=20)\n",
        "        rew_y, rew_x = np.where(self.maze == \"G\")\n",
        "        ax.plot(rew_x, rew_y, \"b*\", ms=30)\n",
        "        return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e6c4292",
      "metadata": {
        "id": "7e6c4292"
      },
      "outputs": [],
      "source": [
        "class MultiQLearner(AbstractAgent):\n",
        "    def __init__(self, env, alpha=0.02, beta=1, gamma=1, epsilon=0.5):\n",
        "        super().__init__(env)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        # Table of Q values with states on the rows and actions on the columns\n",
        "        self.Q_table = [[0.0 for j in range(self.env.get_num_actions())] for i in range(self.env.get_num_states())]\n",
        "        self.epsilon = epsilon  # exploration probability\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "        #TODO implement the Q-Learning algorithm to train the agent\n",
        "        # get current state\n",
        "        state = self.env.get_current_state()\n",
        "        state_index = self.env.get_state_space().index(state)\n",
        "\n",
        "        # TODO select an action by epsilon greedy approach\n",
        "        action =\n",
        "\n",
        "        # TODO perform the action and compute the reward\n",
        "        reward = self.env.step(action)\n",
        "\n",
        "\n",
        "        # TODO update q table\n",
        "        self.Q_table[state_index][action] =\n",
        "\n",
        "        return action, reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4274f519",
      "metadata": {
        "id": "4274f519"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.animation\n",
        "import IPython.display\n",
        "\n",
        "\n",
        "class RLRunner:\n",
        "\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def step(self):\n",
        "        # TODO make the agent perform an action and record the reward\n",
        "\n",
        "    def run(self, n_steps):\n",
        "        #TODO train the agent for n_steps\n",
        "\n",
        "    def plot_cumulative_rewards(self):\n",
        "        fig, ax = plt.subplots(1, 1)\n",
        "        #TODO plot all the rewards obtained so far\n",
        "        ax.plot()\n",
        "        ax.set_xlabel(\"Step\")\n",
        "        ax.set_ylabel(\"Rewards (cuml)\")\n",
        "\n",
        "    def plot_reward_rate(self, window_size=100):\n",
        "        fig, ax = plt.subplots(1, 1)\n",
        "        #TODO plot rewards obtained for every \"window_size\"\n",
        "\n",
        "        ax.set_xlabel(\"Step\")\n",
        "        ax.set_ylabel(\"Reward/step\")\n",
        "\n",
        "    def plot_max_value_per_state(self):\n",
        "        #Plot max value per state as a heat map\n",
        "\n",
        "\n",
        "    def animate_history(self,fps=20, frames=None,):\n",
        "        if frames is None:\n",
        "            frames = len(self.states)\n",
        "        fig = self.agent.env.render()\n",
        "\n",
        "        def anim_update(i):\n",
        "            y, x = self.states[i]\n",
        "            self.agent.env.agent_dot.set_data((x,), (y,))\n",
        "\n",
        "        anim = matplotlib.animation.FuncAnimation(fig, anim_update,\n",
        "                                                  frames=frames,\n",
        "                                                  interval=1000.0 / fps)\n",
        "\n",
        "        #TODO add your ffmpeg path from anaconda\n",
        "        plt.rcParams['animation.ffmpeg_path'] = \"/home/miniconda3/bin/ffmpeg\"\n",
        "\n",
        "        video = anim.to_html5_video()\n",
        "        plt.close(fig)\n",
        "        with open(\"myvideo.html\", \"w\") as f:\n",
        "            print(video, file=f)\n",
        "\n",
        "        #return IPython.display.HTML(video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c142fb",
      "metadata": {
        "id": "b8c142fb"
      },
      "outputs": [],
      "source": [
        "if __name__=='__main__':\n",
        "    #TODO create a grid\n",
        "    #TODO create an agent\n",
        "    #TODO train the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e3bba47",
      "metadata": {
        "id": "2e3bba47"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}